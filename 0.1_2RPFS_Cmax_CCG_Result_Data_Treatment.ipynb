{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2RPFS Problem (Cmax objective) - Data treatment of result files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in the output folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootfolder = os.getcwd()\n",
    "file_list = glob.glob(os.path.join(rootfolder, 'output') + '/*.csv', recursive=True)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative script to treat files with incorrect number of coluns or faulty lines\n",
    "def alternative_csv_reader(filename, delimiter=',', header=0, names=None):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines() \n",
    "        count = 1\n",
    "        line_list = []\n",
    "        num_columns = 20\n",
    "        for line in lines:  # Strips the newline character \n",
    "            #print(\"line{}: {}\".format(count, line.strip())) \n",
    "            nc = len(line.split(','))\n",
    "            if 'executionId,' in line:\n",
    "                #num_columns = nc\n",
    "                print('Detected {0} columns in CSV file.'.format(nc))\n",
    "            else:\n",
    "                if 'none,' in line:\n",
    "                    if nc == num_columns:\n",
    "                        line_list.append(line)\n",
    "                    elif nc > num_columns:  # treat strange truncated lines\n",
    "                        line = line[line.rfind('none,'):]\n",
    "                        nc = len(line.split(','))\n",
    "                        if nc == num_columns:\n",
    "                            print('WARN: truncating line {0}, for having more columns than expected.'.format(count))\n",
    "                            line_list.append(line)\n",
    "                        else:\n",
    "                            print('WARN: Ignoring line {0}, since it has {1} columns, instead of {2}: '.format(count, nc, num_columns), line)\n",
    "                    else:  # Ignore line\n",
    "                        print('WARN: Ignoring line {0}: '.format(count), line)    \n",
    "                elif len(line_list[-1].split(',')) < num_columns:  # current line is a continuation of the previous one\n",
    "                    line_list[-1] = line_list[-1].replace('\\n', '') + line\n",
    "                    print('*** Treated line {0}: '.format(count), line_list[-1])\n",
    "                else:  # Ignore line\n",
    "                    print('WARN: Ignoring line {0}: '.format(count), line)\n",
    "            count += 1\n",
    "        # assert all lines have the same number of columns\n",
    "        count = 1\n",
    "        for line in line_list:\n",
    "            nc = len(line.split(','))\n",
    "            if nc != num_columns:\n",
    "                print('ERROR: Line {0} has {1} columns, instead of {2}: '.format(count, nc, num_columns), line)\n",
    "            count += 1\n",
    "        text_data = StringIO(''.join(line_list))\n",
    "        #print('line_list: ', str(line_list))\n",
    "        #print('text_data: ', text_data)\n",
    "        df = pd.read_csv(text_data, delimiter=delimiter, header=header, names=names)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all CSV files and append all data to a single dataframe (one per solution method: Wilson, Wagner) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_wilson = pd.DataFrame()\n",
    "df_wagner = pd.DataFrame()\n",
    "for filename in file_list:\n",
    "    print('Processing file ', filename)\n",
    "    try:\n",
    "        df_ = pd.read_csv(filename, delimiter=',', header=0, names=['executionId','ub_name','instance_name','alpha','n','m','budget_Gamma','cmax','permutation','time_spent','time_to_best_sol','iterations','num_visited_solutions','num_improvements','is_optimal','validated','gap','lb','cost','cmax_dp'])\n",
    "    except:  # try alternative method to read csv lines\n",
    "        df_ = alternative_csv_reader(filename, delimiter=',', header=0, names=['executionId','ub_name','instance_name','alpha','n','m','budget_Gamma','cmax','permutation','time_spent','time_to_best_sol','iterations','num_visited_solutions','num_improvements','is_optimal','validated','gap','lb','cost','cmax_dp'])\n",
    "    if 'wagner' not in filename:  # Wilson result file\n",
    "        df_wilson = df_wilson.append(df_.copy())\n",
    "    else:  # Wagner result file\n",
    "        df_wagner = df_wagner.append(df_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wilson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wagner.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated header rows from both dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_invalid_values(df):\n",
    "    all_invalid_values = set()\n",
    "    for col in df:\n",
    "        if col not in ['executionId','ub_name','instance_name','budget_Gamma','permutation','is_optimal','validated']:\n",
    "            # 'alpha','n','m','cmax','time_spent','time_to_best_sol','iterations','num_visited_solutions','num_improvements','gap','lb','cost','cmax_dp'\n",
    "            a = pd.to_numeric(df[col], errors='coerce')\n",
    "            idx = a.isna()\n",
    "            invalid_values = df.loc[idx][col].unique()\n",
    "            all_invalid_values.update(invalid_values)\n",
    "        #elif col in ['is_optimal','validated']\n",
    "    print('Invalid values:', all_invalid_values)\n",
    "    return all_invalid_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_invalid_values(df_wagner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_invalid_values(df):\n",
    "    # IMPORTANT: AVOID FILTERING 'NAN' VALUES\n",
    "    for invalid_value in find_invalid_values(df):\n",
    "        if isinstance(invalid_value, str):  # Evita filtrar os nan\n",
    "            df = df[~(df == invalid_value).any(axis=1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wilson = filter_invalid_values(df_wilson)\n",
    "find_invalid_values(df_wilson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wagner = filter_invalid_values(df_wagner)\n",
    "find_invalid_values(df_wagner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert column types from object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_types(df):\n",
    "    for col in df:\n",
    "        if col in ['alpha','n','m','cmax','time_spent','time_to_best_sol','iterations','num_visited_solutions','num_improvements','gap','lb','cost','cmax_dp']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        elif col in ['is_optimal','validated']:\n",
    "            df[col] = df[col].astype('bool')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_wilson = convert_column_types(df_wilson)\n",
    "df_wagner = convert_column_types(df_wagner)\n",
    "df_wagner.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim existing string columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_all_columns(df):\n",
    "    \"\"\"\n",
    "    Trim whitespace from ends of each value across all series in dataframe\n",
    "    \"\"\"\n",
    "    trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n",
    "    return df.applymap(trim_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wilson = trim_all_columns(df_wilson)\n",
    "df_wagner = trim_all_columns(df_wagner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include a column with the name of the underlying C&CG MILP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wilson['model'] = 'Wilson'\n",
    "df_wagner['model'] = 'Wagner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_wilson.append(df_wagner)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the column budget_Gamma into Gamma1 and Gamma2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data frame with split value columns \n",
    "new = df[\"budget_Gamma\"].str.split(\" \", n = 1, expand = True) \n",
    "# making separate first name column from new data frame \n",
    "df[\"Gamma1\"]= new[0] \n",
    "# making separate last name column from new data frame \n",
    "df[\"Gamma2\"]= new[1] \n",
    "# convert Gamma columns to numeric\n",
    "df[\"Gamma1\"] = pd.to_numeric(df[\"Gamma1\"], errors='coerce')\n",
    "df[\"Gamma2\"] = pd.to_numeric(df[\"Gamma2\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix instance names \n",
    "\n",
    "The original instance names, as provided by Ying in the instance file zip, were assembled incorrectly.\n",
    "\n",
    "The problem lies in the alpha percentage. We are now going to fix this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq'] = df['instance_name'].str[7:9]\n",
    "df['n_str'] = df['n'].astype(str).str.zfill(3)\n",
    "df['alpha_str'] = df['alpha'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['instance_name'] = 'RB' + df['n_str'] + df['alpha_str'] + df['seq'] + '.txt'\n",
    "df.drop(columns=['n_str', 'alpha_str'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round columns containing time (in seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_spent'] = df['time_spent'].round(2)\n",
    "df['time_to_best_sol'] = df['time_to_best_sol'].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort data according to model, instance_name, alpha, n, m, Gamma1 and Gamma2 and set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorting dataset...')\n",
    "df = df.sort_values(['model', 'n', 'm', 'alpha', 'instance_name', 'Gamma1', 'Gamma2'])\n",
    "display(df.dtypes)\n",
    "df = df.set_index(['model', 'n', 'm', 'alpha', 'instance_name', 'Gamma1', 'Gamma2'])\n",
    "display(df.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing results, for a given value of alpha, n and m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given group of alpha, n, m and budget_Gamma, there should be 10 results.\n",
    "\n",
    "First we will build a dataframe with the instances list and all required budget values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "rootfolder = os.getcwd()\n",
    "jobs_folders = glob.glob(os.path.join(rootfolder, 'instances', 'robust', 'ying', 'data', '*/'), recursive=False)\n",
    "for job_path in jobs_folders:\n",
    "    alpha_folders = glob.glob(os.path.join(job_path, '*/'), recursive=False)\n",
    "    n = job_path[job_path.find('data')+5:job_path.rfind('jobs')]\n",
    "    #print('n: {}'.format(n))\n",
    "    for alpha_path in alpha_folders:\n",
    "        alpha = alpha_path[alpha_path.find('jobs')+5:alpha_path.rfind('%')]\n",
    "        #print('alpha: {}'.format(alpha))\n",
    "        instance_paths = glob.glob(os.path.join(alpha_path, '*'), recursive=False)\n",
    "        for instance_path in instance_paths:\n",
    "            instance_name = instance_path[instance_path.find('%')+2:]\n",
    "            #print(instance_name)\n",
    "            for gamma1 in [20, 40, 60, 80, 100]:\n",
    "                for gamma2 in [20, 40, 60, 80, 100]:\n",
    "                    for model in ['Wilson', 'Wagner']:\n",
    "                        data.append([model, instance_name, alpha, n, 2, gamma1, gamma2])\n",
    "df_instances = pd.DataFrame(data, columns=['model', 'instance_name', 'alpha', 'n', 'm', 'Gamma1', 'Gamma2'])\n",
    "for col in df_instances:\n",
    "    if col in ['alpha','n','m','Gamma1','Gamma2']:\n",
    "        df_instances[col] = pd.to_numeric(df_instances[col], errors='coerce')\n",
    "display(df_instances.dtypes)\n",
    "df_instances = df_instances.set_index(['model', 'n', 'm', 'alpha', 'instance_name', 'Gamma1', 'Gamma2'])\n",
    "display(df_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets join the instances dataframe with the results one (left join)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_instances.join(df, how='left')\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will export to CSV a list with all rows with NaN values (missing experimental results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = df_joined[df_joined.isnull().any(axis=1)].reset_index()[['model', 'n', 'm', 'alpha', 'instance_name', 'Gamma1', 'Gamma2']]\n",
    "outputfolder = os.path.join(os.getcwd(), 'results', 'consolidated')\n",
    "if not os.path.exists(outputfolder):\n",
    "    os.makedirs(outputfolder)\n",
    "print('Saving file on folder: ' + outputfolder)\n",
    "fname = os.path.join(outputfolder, '2RPFS_Cmax_missing_results.csv')\n",
    "missing_df.to_csv(fname, sep=';')\n",
    "print('Saved: ' + fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(['alpha', 'n', 'm', 'budget_Gamma']).agg({'executionId' : ['count']}).reset_index()\n",
    "df_grouped.columns = [ ' '.join(str(i) for i in col) for col in df_grouped.columns]\n",
    "#df_grouped.reset_index(inplace=True)\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(df, values='executionId', index=['alpha', 'n'], columns=['Gamma1', 'Gamma2'], aggfunc='count', fill_value=0)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "    display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the dataset to CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Saving file on folder: ' + outputfolder)\n",
    "fname = os.path.join(outputfolder, '2RPFS_Cmax_all_results.csv')\n",
    "df.to_csv(fname, sep=';')\n",
    "print('Saved: ' + fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
